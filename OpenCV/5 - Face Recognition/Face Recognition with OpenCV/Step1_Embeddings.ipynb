{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d19998",
   "metadata": {},
   "source": [
    "# Extracting Embeddings using OpenCV\n",
    "\n",
    "This notebook is based on the [OpenCV Face Recognition by Pyimagesearch](https://www.pyimagesearch.com/2018/09/24/opencv-face-recognition/).\n",
    "\n",
    "OpenCV offers to us a piratical module, the dnn (Deep Neural Network module). Among the several functions, we have the readNetFromTorch, that uses  the Torch7 framework's format to read pre-trained models. This example considers the FaceNet model by [Schroff et al](https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/1A_089.pdf), pre-trained in Torch.\n",
    "\n",
    "The FaceNet model considers a CNN model (read the paper for more details) to recognize and cluster the images. In the end of the model, we have a $128-d$ face embedding vector with image features. The model also considers the  Triplet Loss Function. This functions takes into account 3 images, the anchor, the positive and negative. To calculate the loss, we have\n",
    "\n",
    "$L = \\sum_i^N\\left[ \\| f\\left( x_i^a\\right) -  f\\left( x_i^p\\right)\\|_2^{2}  -\\| f\\left( x_i^a\\right) -  f\\left( x_i^n\\right)\\|_2^{2}  + \\alpha \\right]$,\n",
    "\n",
    "where $f\\left( x\\right)$ is the embedding representation for the anchor, positive and negative images, the term $\\alpha$ is a margin that is enforced between positive and negative pairs. Using the Triplet loss function, the model tweaks the weighs according this condition $\\| f\\left( x_i^a\\right) -  f\\left( x_i^p\\right)\\|_2^{2} + \\alpha < \\| f\\left( x_i^a\\right) -  f\\left( x_i^n\\right)\\|_2^{2}$, that means the error between the anchor and positive images is smaller than the error between the anchor and negative images.\n",
    "\n",
    "The dataset is composed by four classes and twenty images for each of them. The images come from three Brazilian singers (Caetano Veloso, Chico Buarque and Gilberto Gil) and a set of other persons labeled into the unknown class.\n",
    "\n",
    "In this example, we do not consider a deep image preprocessing, the idea is to deploy a simple and directly model (a training of how to do). There is other example that we consider the dlib library and facenet, these libraries offers to us a better image preprocessing. Thanks PyImage for it. Let's go on ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b45641d",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa92a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec7f152",
   "metadata": {},
   "source": [
    "## Loading the pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a458dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the face detector\n",
    "protoPath = os.path.sep.join([\"input\", \"deploy.prototxt.txt\"])\n",
    "modelPath = os.path.sep.join([\"input\", \"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d2c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the pre trained FaceNet model\n",
    "embedder = cv2.dnn.readNetFromTorch(\"input/nn4.small2.v1.t7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20d3fc",
   "metadata": {},
   "source": [
    "## Setting the images paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63dd1925",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = list(paths.list_images(\"Dataset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72995c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of feature vector and labels\n",
    "Embeddings = []\n",
    "Names = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818599c",
   "metadata": {},
   "source": [
    "## Embeddings the faces\n",
    "\n",
    "All images in the dataset contain just one face of each class. To obtain the embeddings vector, the main loop follow these steps\n",
    "- Start the loop and detect the face in the image\n",
    "- Once we have the detection, we extract the ROI and crop the image\n",
    "- Finally, we extract the features with the embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f57f5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de135e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    name = imagePath.split(os.path.sep)[-2] # grabbing the label\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=600) # resize\n",
    "    (h, w) = image.shape[:2]\n",
    "    \n",
    "    # construct a blob from the image\n",
    "    imageBlob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "                                      (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "    # apply OpenCV's deep learning-based face detector to localize\n",
    "    # faces in the input image\n",
    "    detector.setInput(imageBlob)\n",
    "    detections = detector.forward()\n",
    "    if len(detections) > 0:\n",
    "        # we're making the assumption that each image has only ONE\n",
    "        # face, so find the bounding box with the largest probability\n",
    "        i = np.argmax(detections[0, 0, :, 2])\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # ensure that the detection with the largest probability also\n",
    "        # means our minimum probability test (thus helping filter out\n",
    "        # weak detections)\n",
    "        if confidence > 0.5:\n",
    "            # compute the (x, y)-coordinates of the bounding box for\n",
    "            # the face\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            # extract the face ROI and grab the ROI dimensions\n",
    "            face = image[startY:endY, startX:endX]\n",
    "            (fH, fW) = face.shape[:2]\n",
    "            # ensure the face width and height are sufficiently large\n",
    "            if fW < 20 or fH < 20:\n",
    "                continue\n",
    "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                                             (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "            embedder.setInput(faceBlob)\n",
    "            vec = embedder.forward()\n",
    "            # add the name of the person + corresponding face\n",
    "            # embedding to their respective lists\n",
    "            Names.append(name)\n",
    "            Embeddings.append(vec.flatten())\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a3a4c",
   "metadata": {},
   "source": [
    "## Saving the features into a pickle data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "548784f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing 78 encodings...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] serializing {} encodings...\".format(total))\n",
    "data = {\"embeddings\": Embeddings, \"names\": Names}\n",
    "f = open(\"output/embeddings1.pickle\", \"wb\")\n",
    "f.write(pickle.dumps(data))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379be698",
   "metadata": {},
   "source": [
    "**Next step, train the classification model**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

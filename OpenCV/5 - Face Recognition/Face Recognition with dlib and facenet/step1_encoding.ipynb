{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1dd9ab1",
   "metadata": {},
   "source": [
    "# Face Recognition with Face Recognition based on Dlib\n",
    "\n",
    "In the last example, we've realized the face recognition using OpenCV, Deep Learning and a classification model. The results was not great, even if we've reached an accuracy around $70\\%$. In this example, we consider the Face Recognition library. This library is built using dlib face recognition, making the process of recognition directly and quickly.\n",
    "\n",
    "The dlib face recognition works similarly the last example. First of all, we extract the faces in the image, we crop them. The next step is to obtain the feature $128d$ vector of each image. Finally, we realize the recognition using a classification model.\n",
    "\n",
    "The advantages to use Face Recognition is, the library uses dlib that is build on C++. When the faces are obtained using the face detection all images are processed, using face landmarks and alignment. This image preprocessing lead the model to very significant results. The Deep Neural Network considered is the ResNet-34, using the pre-trained model trained on  the Labeled Faces in the Wild (LFW) dataset. The classification model considered by the library is the Support Vector Machine.\n",
    "\n",
    "Here, we present the first step of the project. We obtain the Embedding vector of all images. The dataset is composes by $3$ Brazilian singers, $20$ photos for each of them. It's recommended to use the GPU if you have. To train without GPU is recommended to use the HOG model, to realize the face detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7abd9f",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa005cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import face_recognition\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c84e8e6",
   "metadata": {},
   "source": [
    "## Setting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b82dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"/home/igor/Documents/Artificial_Inteligence/Formation/Computer Vision Training/5 - Face Recognition/Face Recognition with dlib and facenet/Dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea27cd5",
   "metadata": {},
   "source": [
    "**List of the paths of all images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4db1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = list(paths.list_images(dataset))\n",
    "# initialize the list of known encodings and known names\n",
    "knownEncodings = [] # list of encondings featues\n",
    "knownNames = [] # List of names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb848f5",
   "metadata": {},
   "source": [
    "## Extracting the vector of features\n",
    "\n",
    "- We extract the name of each person and put it inside the list name\n",
    "- We read the image and change the channels to RGB order\n",
    "- Using the face recognition we detect the face and save the coordinates\n",
    "- We encode all the faces and put them into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08cf79d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processing image 10/60\n",
      "[INFO] processing image 20/60\n",
      "[INFO] processing image 30/60\n",
      "[INFO] processing image 40/60\n",
      "[INFO] processing image 50/60\n",
      "[INFO] processing image 60/60\n"
     ]
    }
   ],
   "source": [
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # extract the person name from the image path\n",
    "    if (i + 1)%10 == 0:\n",
    "        print(\"[INFO] processing image {}/{}\".format(i + 1, len(imagePaths)))\n",
    "    name = imagePath.split(os.path.sep)[-2]\n",
    "    # load the input image and convert it from BGR (OpenCV ordering)\n",
    "    # to dlib ordering (RGB)\n",
    "    image = cv2.imread(imagePath)\n",
    "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # detect the (x, y)-coordinates of the bounding boxes\n",
    "    # corresponding to each face in the input image\n",
    "    boxes = face_recognition.face_locations(rgb, model=\"output/encodings.pickle\")\n",
    "    # the argument model is the directory to store all features in the disk\n",
    "    encodings = face_recognition.face_encodings(rgb, boxes)\n",
    "    for encoding in encodings:\n",
    "        # add each encoding + name to our set of known names and\n",
    "        # encodings\n",
    "        knownEncodings.append(encoding)\n",
    "        knownNames.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be6188",
   "metadata": {},
   "source": [
    "## Writting the embedded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bdc52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"encodings\": knownEncodings, \"names\": knownNames} # dictionary\n",
    "f = open(\"output/encodings.pickle\", \"wb\") # open the pickle file\n",
    "f.write(pickle.dumps(data)) # writting the elements\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

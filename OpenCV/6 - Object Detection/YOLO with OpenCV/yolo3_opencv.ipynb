{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32183f5a",
   "metadata": {},
   "source": [
    "# Detecting objects with YOLO3 on Coco dataset\n",
    "\n",
    "YOLO (you only look once) is a model of object detection proposed by [Redmon et al. in 2015](https://pjreddie.com/media/files/papers/yolo_1.pdf). From your first version, YOLO had modification in your backbone, nowadays the YOLO family counts with five different versions (the two last version do not come from the first authors, [YOLOv4](https://arxiv.org/pdf/2004.10934.pdf) and [YOLOv5](https://github.com/ultralytics/yolov5)). \n",
    "\n",
    "YOLO is an unified model, it means, the model consider one single Convolutional Neural Network  to detect objects and classify them. Before YOLO, there were some models of object detection, but these models detect object in two step, first they found the possible bounding box and then, they make your class classification. \n",
    "\n",
    "YOLO has changed the way to detect object, becoming one of the most powerful real-time object detection, more stronger and faster than other models as SSD or Faster R-CNN. On the other hand, YOLO might not work well for small objects and, the accuracy sometimes tends to worse in comparison with SDD and Faster R-CNN. \n",
    "\n",
    "This notebook do not intends to give a detailed explanation about how the model is built and trained, on the contrary, here we consider a pre-trained [YOLOv3 from DarkNet](https://pjreddie.com/media/files/papers/YOLOv3.pdf) to make prediction over the classes contained in the COCO dataset, where the model was trained. The main objective is to explore the trained model and, use OpenCV to print out the detection on images and video streaming.\n",
    "\n",
    "**OBS.:** We do not find the file with the YOLOv3 weights on this GitHub repository due to your size extension, to download it, [click here](https://pjreddie.com/media/files/yolov3.weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4eef01",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0939513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f976597",
   "metadata": {},
   "source": [
    "## Setting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3c9be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the COCO class labels that our YOLO model was trained on\n",
    "yolo = \"yolo-coco\"\n",
    "labelsPath = os.path.sep.join([yolo, \"coco.names\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "#Coco dataset has 80 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0db4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the color list to represent each possible class label\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\") # color to the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83aca353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = os.path.sep.join([yolo, \"yolov3.weights\"])\n",
    "configPath = os.path.sep.join([yolo, \"yolov3.cfg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7ffec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath) # defining the model with opencv dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "686d8b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the paths for all images to be tested\n",
    "dataset = \"../dataset/images\"\n",
    "pathImages = list(paths.list_images(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f8bf84",
   "metadata": {},
   "source": [
    "## Main loop over the image\n",
    "\n",
    "The Object Detection with a pre-trained model on OpenCV is made by some steps\n",
    "\n",
    "- Read the image and grab the height and width\n",
    "- Get the final layer of the YOLOv3 to make prediction\n",
    "- Pass the input image to extract the blobs (images transformation for the detection) and set it in the model\n",
    "- Make the predictions with the attribute forward, this returns all prediction made by the model\n",
    "- Loop to extract all detection score, classID and confidence associated to a certain class\n",
    "- If condition to select the highest confidences, extract the coordinates of the bounding box, normalize the coordinates, creating lists of boxes, confidences and classID.\n",
    "- Once we have all detections, we consider NomMax suppress to eliminate the boxes that exceeds, selecting the box with the highest IOU score\n",
    "- From the list gave by the NonMax suppress, if there's considerable detection, we extract the box coordinates and print it on the image\n",
    "- Finally, we visualize the detection with your respective bounding box and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b50a4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(120, 120))\n",
    "count = 1\n",
    "for img in pathImages:\n",
    "    \n",
    "    image = cv2.imread(img)\n",
    "    (H, W) = image.shape[:2]\n",
    "    # Determine only the output layer names that we need from YOLO\n",
    "    ln = net.getLayerNames()\n",
    "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    # Construct a blob from the input image, perform a forward pass of the YOLO object detector and that will give us\n",
    "    # bounding boxes alongside its associated probabilities\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(ln)\n",
    "    # Show timing information on YOLO\n",
    "    # Initialize the list of detected bounding boxes, confidences and class IDs respectively\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    classIDs = []\n",
    "    # Loop over each one of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each one of the detections\n",
    "        for detection in output:\n",
    "            # extract the class ID and confidence (i.e, probability) of the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "            # filter out weak predictions by ensuring the detected probabilityy is greater than the minimum probability\n",
    "            if confidence > 0.5:\n",
    "                # scale the bounding box coordinates back relative to the size of the image, keepin in mind that YOLO\n",
    "                # actually returns the center (x,y) coordinates of the bounding box followed by the boxes width and height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                # use the center (x,y) coordinates to derive the top and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                # update the list of bounding box coordinates, confidences and class IDs\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                classIDs.append(classID)\n",
    "   # Apply non-maxima suppression to suppress weak, overlapping bounding boxes\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.3)\n",
    "    # Ensure at least on detection exists\n",
    "    if len(idxs) > 0:\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in idxs.flatten():\n",
    "            # extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "            # draw a bounding box rectangle and label on the image\n",
    "            color = [int(c) for c in COLORS[classIDs[i]]]\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "            text = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
    "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "    ax = plt.subplot(4, 3, count)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    count += 1\n",
    "    #If you want to visualize with OpenCV, uncoment the two above lines\n",
    "    #cv2.imshow(\"prediction\", image)\n",
    "    #cv2.waitKey(0)\n",
    "plt.savefig(\"detections.jpg\")\n",
    "plt.show()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4849b1",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Here, we've presented twelve images to detection, the results are very accurate in general. YOLO is fast and strong, evidently there's your drawbacks and limitations. For example, in the image number 11, there's some small objects that were note recognized by the model, this is a problem with the YOLOv3. Other problem is, when we have some objects very closed, the model has difficult to performs well, as the last example image. \n",
    "\n",
    "On the other hand, the good predictions are predominant over all images, if we look with attention, the image with the two little boys, one of them is clearly blur, but the model recognized him. YOLO, in my opinion is the better model for object detection, evidently we must consider other models, but for me, I'll look for other models when YOLO does not work well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
